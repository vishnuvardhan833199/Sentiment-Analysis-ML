{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5118bdb2",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Project using Machine Learning\n",
    "This project performs sentiment analysis on text data using NLP and machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14423d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805eabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813f4cd",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8209c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset from NLTK movie reviews (or replace with your dataset)\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball', 'sci.med'], shuffle=True, random_state=42)\n",
    "df = pd.DataFrame({'text': data.data, 'label': data.target})\n",
    "df['label'] = df['label'].map({0: 'Negative', 1: 'Positive'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90dd754",
   "metadata": {},
   "source": [
    "## 3. Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "df[['text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d6c89",
   "metadata": {},
   "source": [
    "## 4. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64141739",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(df['clean_text']).toarray()\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb532152",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c72346",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ee05b",
   "metadata": {},
   "source": [
    "## 6. Train Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d08eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fd64d",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0224264",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c05327",
   "metadata": {},
   "source": [
    "## 8. Predict Custom Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    cleaned = preprocess_text(text)\n",
    "    vector = tfidf.transform([cleaned]).toarray()\n",
    "    prediction = model.predict(vector)\n",
    "    return prediction[0]\n",
    "\n",
    "sample = \"The new medicine has helped me recover fast.\"\n",
    "print(\"Prediction:\", predict_sentiment(sample))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}